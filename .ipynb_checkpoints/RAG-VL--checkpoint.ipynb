{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3d8a968-5838-4d78-9527-b9b1c7dbd682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r \"C:\\Users\\ilang\\Downloads\\requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6022171-8ebc-4a7a-8537-e9cbd451d693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.8.0+cpu)\n",
      "Requirement already satisfied: torchvision in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.23.0+cpu)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.8.0+cpu)\n",
      "Requirement already satisfied: filelock in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (74.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\ilang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\ilang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\ilang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21b66b1c-bcb7-4ed5-babe-9b77e5169f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ftfy in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (6.3.1)\n",
      "Requirement already satisfied: regex in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ftfy) (0.2.13)\n",
      "Requirement already satisfied: colorama in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\ilang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\ilang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\ilang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#pip install ftfy regex tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2666765c-4438-4464-acdb-9eb292a82c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"git --version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fc64a13-08eb-4818-ab10-20f619e56c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\ilang\\appdata\\local\\temp\\pip-req-build-xav1ktkm\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: ftfy in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from clip==1.0) (6.3.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from clip==1.0) (24.1)\n",
      "Requirement already satisfied: regex in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from clip==1.0) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from clip==1.0) (4.66.5)\n",
      "Requirement already satisfied: torch in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from clip==1.0) (2.8.0+cpu)\n",
      "Requirement already satisfied: torchvision in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from clip==1.0) (0.23.0+cpu)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->clip==1.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->clip==1.0) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->clip==1.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->clip==1.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->clip==1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->clip==1.0) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->clip==1.0) (74.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision->clip==1.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision->clip==1.0) (10.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->clip==1.0) (0.4.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (pyproject.toml): started\n",
      "  Building wheel for clip (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369633 sha256=6d6658920366614409475f3c063b5383a7eda4289eb88b0ecfd5b0b1f8aa3ef7\n",
      "  Stored in directory: C:\\Users\\ilang\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-vsqdj3xu\\wheels\\35\\3e\\df\\3d24cbfb3b6a06f17a2bfd7d1138900d4365d9028aa8f6e92f\n",
      "Successfully built clip\n",
      "Installing collected packages: clip\n",
      "Successfully installed clip-1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\ilang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\ilang\\AppData\\Local\\Temp\\pip-req-build-xav1ktkm'\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\ilang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\ilang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0899459d-de1f-4c82-9649-6981fcb57a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.9.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from faiss-cpu) (24.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\ilang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\ilang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\ilang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf88efa4-cb85-4d35-826f-178cec4fd461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import tabula\n",
    "import faiss\n",
    "import json\n",
    "import base64\n",
    "import pymupdf\n",
    "import requests\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from botocore.exceptions import ClientError\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a37f6afb-fa8b-4ef2-b623-a5e8664a9dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully: data\\attention_paper.pdf\n"
     ]
    }
   ],
   "source": [
    "# Downloading the dataset - URL of the \"Attention Is All You Need\" paper (Replace it with the URL of the PDF file/dataset you want to download)\n",
    "url = \"https://arxiv.org/pdf/1706.03762.pdf\"\n",
    "\n",
    "# Set the filename and filepath\n",
    "filename = \"attention_paper.pdf\"\n",
    "filepath = os.path.join(\"data\", filename)\n",
    "\n",
    "# Create the data directory if it doesn't exist\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Download the file\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open(filepath, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"File downloaded successfully: {filepath}\")\n",
    "else:\n",
    "    print(f\"Failed to download the file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0677180-f219-4ef7-bd82-3e55650bb298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"600\"\n",
       "            src=\"data\\attention_paper.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1ac96e31340>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the PDF file\n",
    "display.IFrame(filepath, width=1000, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bbf4f34-1087-4073-86df-cd2af500d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directories\n",
    "def create_directories(base_dir):\n",
    "    directories = [\"images\", \"text\", \"tables\", \"page_images\"]\n",
    "    for dir in directories:\n",
    "        os.makedirs(os.path.join(base_dir, dir), exist_ok=True)\n",
    "\n",
    "# Process tables\n",
    "def process_tables(doc, page_num, base_dir, items):\n",
    "    try:\n",
    "        tables = tabula.read_pdf(filepath, pages=page_num + 1, multiple_tables=True)\n",
    "        if not tables:\n",
    "            return\n",
    "        for table_idx, table in enumerate(tables):\n",
    "            table_text = \"\\n\".join([\" | \".join(map(str, row)) for row in table.values])\n",
    "            table_file_name = f\"{base_dir}/tables/{os.path.basename(filepath)}_table_{page_num}_{table_idx}.txt\"\n",
    "            with open(table_file_name, 'w') as f:\n",
    "                f.write(table_text)\n",
    "            items.append({\"page\": page_num, \"type\": \"table\", \"text\": table_text, \"path\": table_file_name})\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting tables from page {page_num}: {str(e)}\")\n",
    "\n",
    "# Process text chunks\n",
    "def process_text_chunks(text, text_splitter, page_num, base_dir, items):\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        text_file_name = f\"{base_dir}/text/{os.path.basename(filepath)}_text_{page_num}_{i}.txt\"\n",
    "        with open(text_file_name, 'w', encoding='utf-8') as f:\n",
    "            f.write(chunk)\n",
    "        items.append({\"page\": page_num, \"type\": \"text\", \"text\": chunk, \"path\": text_file_name})\n",
    "\n",
    "# Process images\n",
    "def process_images(page, page_num, base_dir, items):\n",
    "    images = page.get_images()\n",
    "    for idx, image in enumerate(images):\n",
    "        xref = image[0]\n",
    "        pix = pymupdf.Pixmap(doc, xref)\n",
    "        image_name = f\"{base_dir}/images/{os.path.basename(filepath)}_image_{page_num}_{idx}_{xref}.png\"\n",
    "        pix.save(image_name)\n",
    "        with open(image_name, 'rb') as f:\n",
    "            encoded_image = base64.b64encode(f.read()).decode('utf8')\n",
    "        items.append({\"page\": page_num, \"type\": \"image\", \"path\": image_name, \"image\": encoded_image})\n",
    "\n",
    "# Process page images\n",
    "def process_page_images(page, page_num, base_dir, items):\n",
    "    pix = page.get_pixmap()\n",
    "    page_path = os.path.join(base_dir, f\"page_images/page_{page_num:03d}.png\")\n",
    "    pix.save(page_path)\n",
    "    with open(page_path, 'rb') as f:\n",
    "        page_image = base64.b64encode(f.read()).decode('utf8')\n",
    "    items.append({\"page\": page_num, \"type\": \"page\", \"path\": page_path, \"image\": page_image})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31fad315-4cff-4030-a498-37783d08a934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDF pages:   0%|                                                                                  | 0/15 [00:00<?, ?it/s]Failed to import jpype dependencies. Fallback to subprocess.\n",
      "No module named 'jpype'\n",
      "Processing PDF pages:  20%|██████████████▊                                                           | 3/15 [00:04<00:20,  1.72s/it]Got stderr: Sept 16, 2025 8:11:20 AM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\n",
      "WARNING: No Unicode mapping for summationtext (80) in font THPNLT+CMEX9\n",
      "\n",
      "Processing PDF pages:  40%|█████████████████████████████▌                                            | 6/15 [00:09<00:13,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting tables from page 5: 'utf-8' codec can't decode byte 0xb7 in position 1027: invalid start byte\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDF pages:  53%|███████████████████████████████████████▍                                  | 8/15 [00:12<00:10,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting tables from page 7: 'utf-8' codec can't decode byte 0xb7 in position 1440: invalid start byte\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDF pages:  60%|████████████████████████████████████████████▍                             | 9/15 [00:13<00:08,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting tables from page 8: 'utf-8' codec can't decode byte 0xd7 in position 2962: invalid continuation byte\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDF pages: 100%|█████████████████████████████████████████████████████████████████████████| 15/15 [00:25<00:00,  1.67s/it]\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from tqdm import tqdm\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "doc = pymupdf.open(filepath)\n",
    "num_pages = len(doc)\n",
    "base_dir = \"data\"\n",
    "\n",
    "# Creating the directories\n",
    "create_directories(base_dir)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=200, length_function=len)\n",
    "items = []\n",
    "\n",
    "# Process each page of the PDF\n",
    "for page_num in tqdm(range(num_pages), desc=\"Processing PDF pages\"):\n",
    "    page = doc[page_num]\n",
    "    text = page.get_text()\n",
    "    process_tables(doc, page_num, base_dir, items)\n",
    "    process_text_chunks(text, text_splitter, page_num, base_dir, items)\n",
    "    process_images(page, page_num, base_dir, items)\n",
    "    process_page_images(page, page_num, base_dir, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d64b999f-1d60-4374-bd51-e007956dc734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page': 0,\n",
       " 'type': 'text',\n",
       " 'text': 'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or',\n",
       " 'path': 'data/text/attention_paper.pdf_text_0_0.txt'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in items if i['type'] == 'text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adb5b3fa-1090-4f45-8a6f-70cd5cc56cf8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'items' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mitems\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'items' is not defined"
     ]
    }
   ],
   "source": [
    "[i for i in items if i['type'] == 'table'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6ed7b29-7b19-4e74-9967-b736308abf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "import torch\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the CLIP model\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "print(\"CLIP model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ce89ef8-1dc6-4d0e-a2d1-56db6d7f7adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity scores between the image and text queries:\n",
      "a golden retriever: 0.3222\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Load the model and preprocessing pipeline\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Define inputs\n",
    "text_inputs = [\"a golden retriever\"]\n",
    "image_path = \"dog2.jpg\"\n",
    "\n",
    "# Preprocess the text\n",
    "text_tokens = clip.tokenize(text_inputs).to(device)\n",
    "\n",
    "# Preprocess the image\n",
    "image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "\n",
    "# Generate embeddings\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text_tokens)\n",
    "\n",
    "# Normalize embeddings\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Compute similarity scores\n",
    "similarity = (image_features @ text_features.T).cpu().numpy()\n",
    "\n",
    "print(\"Similarity scores between the image and text queries:\")\n",
    "for i, text in enumerate(text_inputs):\n",
    "    print(f\"{text}: {similarity[0][i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12ef0eb6-16a1-41bd-9fe3-14d2e0cfa224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 nearest neighbors: [[13262 17620 36245 39869 60409]]\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Create a random dataset of 100,000 vectors (128 dimensions)\n",
    "d = 128  # Dimension of vectors\n",
    "nb = 100000  # Number of vectors\n",
    "np.random.seed(1234)\n",
    "data = np.random.random((nb, d)).astype('float32')\n",
    "\n",
    "# Build a FAISS index (Flat index for exact search)\n",
    "index = faiss.IndexFlatL2(d)  # L2 distance\n",
    "index.add(data)  # Add vectors to the index\n",
    "\n",
    "# Query the index\n",
    "query = np.random.random((1, d)).astype('float32')  # Single query vector\n",
    "k = 5  # Number of nearest neighbors to retrieve\n",
    "distances, indices = index.search(query, k)\n",
    "\n",
    "print(f\"Top {k} nearest neighbors: {indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d77724-b744-486b-9ee0-9b71d2d998ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nearest_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m[indices[\u001b[38;5;241m0\u001b[39m]] \n\u001b[0;32m      2\u001b[0m nearest_vectors\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "nearest_vectors = data[indices[0]] \n",
    "nearest_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46762044-8901-43af-8532-7172fa26155e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.8.0+cpu)\n",
      "Requirement already satisfied: torchvision in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.23.0+cpu)\n",
      "Requirement already satisfied: Pillow in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (10.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.34.5-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (74.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ilang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.56.1-py3-none-any.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.6 MB 1.0 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 1.0/11.6 MB 1.6 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 1.8/11.6 MB 2.1 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 3.1/11.6 MB 3.0 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 5.8/11.6 MB 4.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 7.6/11.6 MB 5.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.6/11.6 MB 5.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.7/11.6 MB 4.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.3/11.6 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 5.3 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.34.5-py3-none-any.whl (562 kB)\n",
      "   ---------------------------------------- 0.0/562.2 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 524.3/562.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 562.2/562.2 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "Downloading tokenizers-0.22.0-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.7/2.7 MB 26.0 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.34.5 safetensors-0.6.2 tokenizers-0.22.0 transformers-4.56.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\ilang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\ilang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\ilang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#pip install transformers torch torchvision Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a823ff9-ba46-45b9-8061-494663c1849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "def generate_multimodal_embeddings(prompt=None, image_path=None,output_embedding_length=384):\n",
    "    \"\"\"\n",
    "    Generate multimodal embeddings using OpenAI's CLIP model.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The text prompt to encode.\n",
    "        image_path (str): The path to the image file to encode.\n",
    "    Returns:\n",
    "        dict: A dictionary containing text and image embeddings (if provided).\n",
    "    \"\"\"\n",
    "    if not prompt and not image_path:\n",
    "        raise ValueError(\"Please provide either a text prompt, an image path, or both as input\")\n",
    "    \n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    # Initialize inputs\n",
    "    inputs = {}\n",
    "\n",
    "    if prompt:\n",
    "        inputs[\"text\"] = prompt\n",
    "\n",
    "    if image_path:\n",
    "        # Open and process the image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs[\"images\"] = image\n",
    "\n",
    "    # Process inputs for the model\n",
    "    inputs = processor(**inputs, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model.get_text_features(input_ids=inputs[\"input_ids\"]) if \"text\" in inputs else None\n",
    "        image_embeddings = model.get_image_features(pixel_values=inputs[\"pixel_values\"]) if \"images\" in inputs else None\n",
    "\n",
    "    # Normalize embeddings for cosine similarity (optional)\n",
    "    if outputs is not None:\n",
    "        text_embeddings = outputs / outputs.norm(dim=-1, keepdim=True)\n",
    "    if image_embeddings is not None:\n",
    "        image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Return the embeddings\n",
    "    return {\n",
    "        \"text_embedding\": text_embeddings.numpy() if prompt else None,\n",
    "        \"image_embedding\": image_embeddings.numpy() if image_path else None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a0effb2-cbd3-4318-8c51-c0313a1c0f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:   0%|                                                                                | 0/104 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "generate_multimodal_embeddings() got an unexpected keyword argument 'output_embedding_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 31\u001b[0m\n\u001b[0;32m     27\u001b[0m counters[item_type] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m item_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# For text or table, use the formatted text representation\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_multimodal_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43moutput_embedding_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_vector_dimension\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# For images, use the base64-encoded image data\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m generate_multimodal_embeddings(image\u001b[38;5;241m=\u001b[39mitem[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m], output_embedding_length\u001b[38;5;241m=\u001b[39membedding_vector_dimension)\n",
      "\u001b[1;31mTypeError\u001b[0m: generate_multimodal_embeddings() got an unexpected keyword argument 'output_embedding_length'"
     ]
    }
   ],
   "source": [
    "# Set embedding vector dimension\n",
    "embedding_vector_dimension = 384\n",
    "\n",
    "# Count the number of each type of item\n",
    "item_counts = {\n",
    "    'text': sum(1 for item in items if item['type'] == 'text'),\n",
    "    'table': sum(1 for item in items if item['type'] == 'table'),\n",
    "    'image': sum(1 for item in items if item['type'] == 'image'),\n",
    "    'page': sum(1 for item in items if item['type'] == 'page')\n",
    "}\n",
    "\n",
    "# Initialize counters\n",
    "counters = dict.fromkeys(item_counts.keys(), 0)\n",
    "\n",
    "# Generate embeddings for all items\n",
    "with tqdm(\n",
    "    total=len(items),\n",
    "    desc=\"Generating embeddings\",\n",
    "    bar_format=(\n",
    "        \"{l_bar}{bar}| {n_fmt}/{total_fmt} \"\n",
    "        \"[{elapsed}<{remaining}, {rate_fmt}{postfix}]\"\n",
    "    )\n",
    ") as pbar:\n",
    "    \n",
    "    for item in items:\n",
    "        item_type = item['type']\n",
    "        counters[item_type] += 1\n",
    "        \n",
    "        if item_type in ['text', 'table']:\n",
    "            # For text or table, use the formatted text representation\n",
    "            item['embedding'] = generate_multimodal_embeddings(prompt=item['text'],output_embedding_length=embedding_vector_dimension) \n",
    "        else:\n",
    "            # For images, use the base64-encoded image data\n",
    "            item['embedding'] = generate_multimodal_embeddings(image=item['image'], output_embedding_length=embedding_vector_dimension)\n",
    "        \n",
    "        # Update the progress bar\n",
    "        pbar.set_postfix_str(f\"Text: {counters['text']}/{item_counts['text']}, Table: {counters['table']}/{item_counts['table']}, Image: {counters['image']}/{item_counts['image']}\")\n",
    "        pbar.update(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
